model_configs:
  checkpoint_path: null
  esm_model_name: facebook/esm2_t33_650M_UR50D
  protein_model_name_or_path: facebook/esm2_t33_650M_UR50D
  proteina_model_name_or_path: facebook/esm2_t33_650M_UR50D
  
  # Model hyperparameters
  model_hyperparameters:
    learning_rate: 0.001
    warmup_steps_ratio: 0.06
    protein_max_seq_len: 2048
    proteina_max_seq_len: 2048
    gradient_accumulation_steps: 32
    projected_size: 256
    projected_dropout: 0.3
    esm_layer: 33
    mean_pool: true

  # PEFT configurations
  peft_configs:
    enabled: true  # Enable PEFT
    protein:
      method: "lora"  # lora, loha, lokr, ia3
      rank: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
      bias: "none"
    proteina:
      method: "lora"
      rank: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
      bias: "none"
    
  # Memory optimization
  memory_efficient: true
  gradient_checkpointing: true
  
  # Training configuration
  loss_function: cosine_mse

dataset_configs:
  dataset_name: PPB-Affinity
  split_method: random
  train_ratio: 0.2

training_configs:
  random_seed: 1234
  device: 0
  patience: 75
  epochs: 100
  batch_size: 1
  outputs_dir: "outputs/lora"